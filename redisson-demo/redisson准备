
- 客户端技术架构 netty 异步编程流
- 分布式对象服务	分布式集合 缓存淘汰
- 分布式锁 
	实现对比 
	缓存锁的实现思路 ing
	单点故障 v
	RedLock算法 v
	其他类型的锁 
- 高可用

- 异步编程模型 promise future 

其他主题： 连接池 执行现成池 

缓存锁存在的问题： 
	http://www.weizijun.cn/2016/03/17/%E8%81%8A%E4%B8%80%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E8%AE%BE%E8%AE%A1/

实现中lua脚本语法：http://www.redis.cn/commands/eval.html


public interface Lock {
	void lock();
	void lockInterruptibly() throws InterruptedException;
	boolean tryLock();
	boolean tryLock(long time, TimeUnit unit) throws InterruptedException;
	void unlock();
	Condition newCondition();
}


# 分布式锁
数据库锁：		
	实现方案 	1. 锁池表（向锁池中插入记录， 锁id唯一） 插入成功的获取到锁； 解锁删除记录
				2. for update 如果要实现行锁需要增加索引 
	缺点			数据库是个单点，挂掉之后服务不可用 - 主备数据库
				锁没有失效时间， 一旦解锁失败， 其他线程再也无法获取锁 - 定时线程解锁
				非阻塞的无法形成队列效果 - 客户端while循环
				不可重入 - 锁定记录增加主机-线程信息
				不灵活
				# 阻塞过程中，严重占用数据库链接
				# 对于较小的表， for update 可能会因为执行计划问题直接锁定全表
				# 数据落盘开销很大
	优点			容易理解

缓存锁：			性能出色，redis可以每秒执行10w次，内网延迟不超过1ms；	
				锁存在与内存中，redis宕机之后锁信息丢失
	优点			集群部署，解决单点问题
				更好的性能
	缺点			使用超时时间控制锁的失效并不靠谱

zookeeper锁：
	缺点 		性能上不如使用缓存实现分布式锁

从实现的复杂性角度（从低到高）Zookeeper >= 缓存 > 数据库
从性能角度（从高到低）缓存 > Zookeeper >= 数据库
从可靠性角度（从高到低）Zookeeper > 缓存 > 数据库

# 缓存锁实现思路：
- redis publish/subscribe : http://redisbook.readthedocs.io/en/latest/feature/pubsub.html
- lua脚本与原子性			：实现  MULTI / EXEC 效果; Redis 使用单个 Lua 解释器去运行所有脚本，并且， Redis 也保证脚本会以原子性(atomic)的方式执行：当某个脚本正在运行的时候，不会有其他脚本或 Redis 命令被执行。这和使用 MULTI / EXEC 包围的事务很类似。在其他别的客户端看来，脚本的效果(effect)要么是不可见的(not visible)，要么就是已完成的(already completed)。
http://redisdoc.com/script/eval.html
- 


# RedLock算法
- 安全可靠性： 
	安全属性：互斥，不管任何时候，只有一个客户端能持有同一个锁。
	效率属性A：不会死锁，最终一定会得到锁，就算一个持有锁的客户端宕掉或者发生网络分区。
	效率属性B：容错，只要大多数Redis节点正常工作，客户端应该都能获取和释放锁。

- 单点故障：
	主从redis切换方案的弊端：（redis复制是异步的）
		客户端A在master节点拿到了锁。
		master节点在把A创建的key写入slave之前宕机了。
		slave变成了master节点 4.B也得到了和A还持有的相同的锁（因为原来的slave里还没有A持有锁的信息

- Redis实现锁的基本思想：
	每个线程使用uuid为标志获取锁， 只有加锁者可以解锁 ，或者锁超过了有效期。
	if redis.call("get",KEYS[1]) == ARGV[1] then
		    return redis.call("del",KEYS[1])
		else
		    return 0
		end

- Redlock算法
	- N * Redis Master
	1. 客户端获取当前时间
	2. 客户端使用相同的key value在N个节点上获取锁； 请求锁超时时间 acquire_timeout , 锁失效时间 TTL 其中 acquire_timeout <<< TTL ; (存在宕机节点后可以迅速的切换到其他节点)
	3. 获取第一把锁的时间t1 ， 获取最后锁的时间t2,  t2 - t1 < TTL 并且获取到节点锁数量超过 n/2 + 1
	4. 如果锁获取成功， redis锁的自动释放时间 TTL - (t2 - t1)
	5. 如果锁获取失败， 客户端向N个节点发送解锁命令(超时问题)

- 死锁重试:
	当一个客户端获取锁失败时，这个客户端应该在一个随机延时后进行重试 （避免活锁），之所以采用随机延时是为了避免不同客户端同时重试导致谁都无法拿到锁的情况出现.这里非常有必要强调一下客户端如果没有在多数节点获取到锁，一定要尽快在获取锁成功的节点上释放锁。 

性能论证：
	1. 锁超时释放， 锁在一定时间内可以被获取
	2. 主动释放锁， 无需等待超时
	3. 随机时间重试，避免死锁

故障恢复：
	1. 如果redis配置非持久话， 同一把锁在redis重启之后可能会被多个节点获取， 违背互斥原则
	2. AOF持久化 没有fsync，每秒fsync，每次请求时fsync http://ifeve.com/redis-persistence/
	只要一个服务节点在宕机重启后不去参与现在所有仍在使用的锁，这样正在使用的锁集合在这个服务节点重启时，算法的安全性就可以维持，因为这样就可以保证正在使用的锁都被所有没重启的节点持有。 为了满足这个条件，我们只要让一个宕机重启后的实例，至少在我们使用的最大TTL时间内处于不可用状态，超过这个时间之后，所有在这期间活跃的锁都会自动释放掉。



15910358633
	

 


